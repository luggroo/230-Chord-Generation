{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9def371f90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author: Yulou Zhou\n",
    "#Weight loss function by commonality (and normalize)\n",
    "#padded sequence\n",
    "#for loop to test parameters\n",
    "#for loop to get prev generated chord\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import csv\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from random import shuffle\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from MyDataset import MyDataset\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p:  tensor([[  5,   4,   7,  10,   3,   7,  10,   3,   7,  10,   3,   7],\n",
      "        [  6,   4,   7,   6,   4,   7,  11,   4,   7,   2,   4,   7],\n",
      "        [  6,   4,   7,   8,   3,   7,   6,   4,   7,   6,   4,   7],\n",
      "        [  6,   2,   7,   1,   7,   0,   8,   4,   7,  10,   3,   7]])\n",
      "m:  tensor([[ 10,  10,  10,   0,   1,   5,  10,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [ 12,  12,  12,  12,  12,  11,  12,  12,  12,  12,  12,  12,\n",
      "          12,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [ 10,  10,  10,  10,   0,   0,   0,   0,  10,  10,  10,  10,\n",
      "           1,   1,   1,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   1,   8,   5,   5,   5,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])\n",
      "l:  tensor([[ 5,  4,  7],\n",
      "        [ 6,  4,  7],\n",
      "        [ 8,  4,  7],\n",
      "        [ 6,  2,  7]])\n",
      "mask:  tensor([  7,  13,  16,   9])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "train_data = MyDataset('relativedata', prevchord = 4)\n",
    "weights = train_data.stats()\n",
    "data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "c=0\n",
    "for i in data_loader:\n",
    "    print(\"p: \", i['p'])\n",
    "    print(\"m: \", i['m'])\n",
    "    print(\"l: \", i['l'])\n",
    "    print(\"mask: \", i['mask'])\n",
    "    c += 10\n",
    "    if c > 0: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(array):\n",
    "    return [i / sum(array) for i in array]\n",
    "newweights = []\n",
    "\n",
    "def weightloss(weights, eps):\n",
    "    for w in weights:\n",
    "        newweights.append(torch.FloatTensor( normalize([1/(i + eps) for i in w] ) ) )\n",
    "    print(newweights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  10000 \n",
      "p_hidden:  16 \n",
      "m_hidden:  43 \n",
      "eps:  0.14212619713184405\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "P_HIDDEN_DIM = max(int(np.random.normal(30, 30)), 5)\n",
    "M_HIDDEN_DIM = max(int(np.random.normal(30, 30)), 5)\n",
    "vocab_size = 13\n",
    "tagset_size = 13\n",
    "EPS = random.uniform(0.005, 0.15)\n",
    "ITER = 10000\n",
    "P_EMBEDDING_DIM = vocab_size\n",
    "M_EMBEDDING_DIM = vocab_size\n",
    "\n",
    "print(\"iter: \", ITER, \"\\np_hidden: \", P_HIDDEN_DIM, \"\\nm_hidden: \", M_HIDDEN_DIM, \"\\neps: \", EPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 0.1083,  0.0468,  0.0985,  0.0589,  0.0788,  0.0805,  0.0526,\n",
      "         0.1018,  0.0502,  0.0907,  0.0684,  0.0690,  0.0956]), tensor(1.00000e-02 *\n",
      "       [ 8.4815,  8.7604,  8.6949,  3.1158,  1.3336,  8.3771,  8.7604,\n",
      "         8.6741,  8.7604,  8.7604,  8.7604,  8.7604,  8.7604]), tensor(1.00000e-02 *\n",
      "       [ 8.1590,  8.5086,  8.5086,  8.4634,  8.5086,  8.5077,  7.3920,\n",
      "         1.0563,  8.1425,  7.2278,  8.5086,  8.5086,  8.5086]), tensor([ 0.1068,  0.0477,  0.0976,  0.0596,  0.0789,  0.0806,  0.0535,\n",
      "         0.1007,  0.0510,  0.0903,  0.0689,  0.0695,  0.0949]), tensor(1.00000e-02 *\n",
      "       [ 8.4771,  8.7395,  8.6779,  3.2311,  1.4004,  8.3787,  8.7395,\n",
      "         8.6584,  8.7395,  8.7395,  8.7395,  8.7395,  8.7395]), tensor(1.00000e-02 *\n",
      "       [ 8.1606,  8.4896,  8.4896,  8.4472,  8.4896,  8.4888,  7.4330,\n",
      "         1.1113,  8.1451,  7.2763,  8.4896,  8.4896,  8.4896])]\n"
     ]
    }
   ],
   "source": [
    "newweishts = weightloss(weights, EPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, p_embed_dim, m_embed_dim, p_hidden_dim, m_hidden_dim, vocab_size, tagset_size, batch_size = 1):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.p_embed_dim, self.m_embed_dim = p_embed_dim, m_embed_dim\n",
    "        #refer to https://discuss.pytorch.org/t/can-we-use-pre-trained-word-embeddings-for-weight-initialization-in-nn-embedding/1222/2\n",
    "        \n",
    "        self.p_embed = nn.Embedding(vocab_size, p_embed_dim)\n",
    "        self.m_embed = nn.Embedding(vocab_size, m_embed_dim)\n",
    "        self.p_embed.weight.data.copy_(torch.from_numpy(np.identity(vocab_size)))\n",
    "        self.m_embed.weight.data.copy_(torch.from_numpy(np.identity(vocab_size)))\n",
    "        \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.p_lstm = nn.LSTM(p_embed_dim, p_hidden_dim, bidirectional = False)\n",
    "        self.m_lstm = nn.LSTM(m_embed_dim, m_hidden_dim, bidirectional = False)\n",
    "        \n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear((p_hidden_dim + m_hidden_dim) , tagset_size)\n",
    "        self.root2second = nn.Linear((p_hidden_dim + m_hidden_dim) + 1, tagset_size)\n",
    "        self.root2third = nn.Linear((p_hidden_dim + m_hidden_dim) + 1, tagset_size)\n",
    "        \n",
    "        self.p_hidden = self.init_hidden(p_hidden_dim)\n",
    "        self.m_hidden = self.init_hidden(m_hidden_dim)\n",
    "\n",
    "    def init_one_hot(self, vocab_size):\n",
    "        #initialize each embedding\n",
    "        #stack them together (or other ways to have  pretrained embeddings)\n",
    "        #pretrained_weight is a numpy matrix of shape (num_embeddings, embedding_dim)\n",
    "        #we should turn data into torch.from_numpy(pretrained_weight)\n",
    "        #embed.weight.data.copy_(torch.from_numpy(pretrained_weight))\n",
    "        \n",
    "        torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "        \n",
    "    \n",
    "    def init_hidden(self, hidden_dim):\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(1, self.batch_size, hidden_dim),\n",
    "                torch.zeros(1, self.batch_size, hidden_dim))\n",
    "\n",
    "    def forward(self, prev, melody, mask):\n",
    "        \n",
    "        prev_embeds = self.p_embed(prev)\n",
    "        x1 = torch.transpose(prev_embeds, 0, 1) #TODO: DEBUG THIS LINE (probably the last batch)\n",
    "        \n",
    "        melody_embeds =  self.m_embed(melody) #Error here\n",
    "        x2 = torch.transpose(melody_embeds, 0, 1)\n",
    "        p_lstm_out, self.p_hidden = self.p_lstm(x1, self.p_hidden)\n",
    "        m_lstm_out, self.m_hidden = self.m_lstm(x2, self.m_hidden)\n",
    "        \n",
    "        p_fstate, m_fstate = p_lstm_out[-1], m_lstm_out[-1]\n",
    "        \n",
    "\n",
    "        concat = torch.cat((p_fstate, m_fstate), 1)\n",
    "        #print(p_fstate.size(), m_fstate.size())\n",
    "        tag_space = self.hidden2tag(concat) #REASON: you only need the final state\n",
    "        tag_scores = F.log_softmax(tag_space, dim = 1)\n",
    "        \n",
    "\n",
    "        withroot = torch.cat((concat, tag_scores.max(1)[1].float().view(-1,1)), 1)\n",
    "        second = self.root2second(withroot)\n",
    "        third = self.root2third(withroot)\n",
    "        second_scores = F.log_softmax(second, dim=1)\n",
    "        third_scores = F.log_softmax(third, dim=1)\n",
    "        \n",
    "        #stacked = torch.stack([tag_scores, second_scores, third_scores], dim=0)\n",
    "\n",
    "        return [tag_scores, second_scores, third_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTagger(P_EMBEDDING_DIM, M_EMBEDDING_DIM, P_HIDDEN_DIM, M_HIDDEN_DIM, vocab_size, tagset_size, batch_size = batch_size)\n",
    "loss_function = nn.NLLLoss(weight = newweights[0])\n",
    "loss_function2 = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p, m, l tensor([[  1,   4,   7,   6,   4,   7,   1,   4,   7,   8,   4,   7],\n",
      "        [  1,   4,   7,  10,   3,   7,   3,   3,   7,   8,   3,   7],\n",
      "        [ 10,   3,   7,   3,   4,   7,   8,   3,   7,   7,   4,   7],\n",
      "        [ 11,   4,   7,   3,   3,   7,   4,   4,   7,   6,   4,   7]]) tensor([[  3,   3,   3,  12,  12,  12,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  8,   8,   8,   6,   6,   6,   6,   6,   6,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  1,   1,   1,   3,   3,   3,   1,   1,   1,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  8,   8,   8,   8,   8,   8,   8,  10,  10,  10,  10,  10,\n",
      "          10,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]]) tensor([[  1,   4,   7],\n",
      "        [ 10,   3,   7],\n",
      "        [  8,   3,   7],\n",
      "        [ 11,   4,   7]])\n",
      "[tensor([[-2.5594, -2.5429, -2.4914, -2.5822, -2.4685, -2.6579, -2.5971,\n",
      "         -2.5119, -2.4838, -2.6763, -2.4310, -2.6534, -2.7395],\n",
      "        [-2.5548, -2.5469, -2.4930, -2.5776, -2.4678, -2.6572, -2.6005,\n",
      "         -2.5145, -2.4985, -2.6764, -2.4321, -2.6566, -2.7144],\n",
      "        [-2.5615, -2.5391, -2.4891, -2.5844, -2.4677, -2.6592, -2.5941,\n",
      "         -2.5142, -2.4817, -2.6766, -2.4337, -2.6529, -2.7415],\n",
      "        [-2.5619, -2.5466, -2.4888, -2.5779, -2.4749, -2.6630, -2.5945,\n",
      "         -2.5107, -2.4795, -2.6783, -2.4262, -2.6585, -2.7355]]), tensor([[-3.2795, -3.4834, -1.9490, -3.7216, -1.8723, -2.7458, -2.3409,\n",
      "         -2.4459, -2.9577, -2.5197, -3.2871, -1.8893, -3.1413],\n",
      "        [-3.2917, -3.4730, -1.9401, -3.7156, -1.8714, -2.7369, -2.3352,\n",
      "         -2.4679, -2.9637, -2.5215, -3.2835, -1.8929, -3.1365],\n",
      "        [-3.2811, -3.4815, -1.9448, -3.7231, -1.8738, -2.7504, -2.3391,\n",
      "         -2.4463, -2.9612, -2.5186, -3.2901, -1.8891, -3.1415],\n",
      "        [-3.2831, -3.4724, -1.9509, -3.7117, -1.8701, -2.7457, -2.3436,\n",
      "         -2.4472, -2.9590, -2.5188, -3.2873, -1.8897, -3.1437]]), tensor([[-2.0277, -1.7660, -1.6446, -2.6558, -1.7941, -2.2923, -4.4117,\n",
      "         -3.8372, -3.4676, -3.6777, -3.7207, -3.3756, -4.0102],\n",
      "        [-2.0258, -1.7536, -1.6629, -2.6498, -1.7947, -2.2954, -4.3950,\n",
      "         -3.8359, -3.4573, -3.6771, -3.7034, -3.3679, -4.0183],\n",
      "        [-2.0264, -1.7710, -1.6448, -2.6583, -1.7917, -2.2884, -4.4121,\n",
      "         -3.8369, -3.4698, -3.6767, -3.7209, -3.3743, -4.0050],\n",
      "        [-2.0249, -1.7705, -1.6452, -2.6499, -1.7992, -2.2855, -4.4065,\n",
      "         -3.8367, -3.4630, -3.6794, -3.7238, -3.3728, -4.0067]])]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i in data_loader:\n",
    "        p, m, l, mask = i['p'], i['m'], i['l'], i['mask']\n",
    "        print(\"p, m, l\", p, m, l)\n",
    "        tag_scores = model(p, m, mask)\n",
    "        print(tag_scores)\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(4.5248)\n",
      "10 tensor(3.2804)\n",
      "20 tensor(2.6759)\n",
      "30 tensor(3.5713)\n",
      "40 tensor(2.2532)\n",
      "50 tensor(2.5057)\n",
      "60 tensor(3.3110)\n",
      "70 tensor(3.9084)\n",
      "80 tensor(3.4953)\n",
      "90 tensor(2.6497)\n",
      "100 tensor(3.0828)\n",
      "110 tensor(3.7762)\n",
      "120 tensor(3.3194)\n",
      "130 tensor(2.8022)\n",
      "140 tensor(2.8333)\n",
      "150 tensor(3.0554)\n",
      "160 tensor(2.6764)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-fa46db49fbf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m0.25\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss_function2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlu/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlu/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(ITER):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for i in data_loader:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.p_hidden = model.init_hidden(P_HIDDEN_DIM)\n",
    "        model.m_hidden = model.init_hidden(M_HIDDEN_DIM)\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        p, m, l, mask = i['p'], i['m'], i['l'], i['mask']\n",
    "        #print(m)\n",
    "        tag_scores = model(p, m, mask)\n",
    "        # Step 3. Run our forward pass.\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        #print(targets[:1])\n",
    "        loss = 0\n",
    "        \n",
    "        loss += loss_function(tag_scores[0], torch.t(l)[0])  \n",
    "        loss += 0.5 * loss_function2(tag_scores[1], torch.t(l)[1])\n",
    "        loss += 0.25 * loss_function2(tag_scores[2], torch.t(l)[2])   \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        break\n",
    "    if(epoch % 10==0): print(epoch, loss)\n",
    "\n",
    "accuracy = 0.0\n",
    "c = 0\n",
    "#print(\"new\")\n",
    "for j in range(100):\n",
    "    for i in data_loader:\n",
    "        c+=1\n",
    "        p, m, l, mask = i['p'], i['m'], i['l'], i['mask']\n",
    "        tag_scores = model(p, m, mask)\n",
    "        #print(tag_scores[0].size())\n",
    "        pred = tag_scores[0].max(1)[1].numpy()\n",
    "        truth = torch.t(l)[0].numpy()\n",
    "        #print((3 == truth))\n",
    "        #print(pred[30:50], truth[30:50])\n",
    "        accuracy += np.average(pred == truth)\n",
    "        if (c>=1): break\n",
    "print(accuracy/c)\n",
    "\n",
    "\n",
    "# See what the scores are after training\n",
    "# with torch.no_grad():\n",
    "#     inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "#     tag_scores = model(inputs)\n",
    "\n",
    "#     # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "#     # for word i. The predicted tag is the maximum scoring tag.\n",
    "#     # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "#     # since 0 is index of the maximum value of row 1,\n",
    "#     # 1 is the index of maximum value of row 2, etc.\n",
    "#     # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "#     _, point2 = tag_scores.max(1)\n",
    "#     print(point2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0.0\n",
    "c = 0\n",
    "#print(\"new\")\n",
    "for j in range(100):\n",
    "    for i in data_loader:\n",
    "        c+=1\n",
    "        p, m, l, mask = i['p'], i['m'], i['l'], i['mask']\n",
    "        tag_scores = model(p, m, mask)\n",
    "        #print(tag_scores[0].size())\n",
    "        pred = tag_scores[0].max(1)[1].numpy()\n",
    "        truth = torch.t(l)[0].numpy()\n",
    "        #print((3 == truth))\n",
    "        #print(pred[30:50], truth[30:50])\n",
    "        accuracy += np.average(pred == truth)\n",
    "        if (c>=1): break\n",
    "print(accuracy/c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
